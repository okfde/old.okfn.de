---
authors: 
- Arne Semsrott
- Walter Palmetshofer
date: 2018-10-31
image:
  src: /files/blog/2018/02/openschufa.png
  title: OpenSchufa
tags:
- OpenSchufa
type: post
layout: post
card: true
published: false
title: "OpenSCHUFA: Verbrauchergerechtes Scoring" 
---
Heute findet die Präsentation des SRVR zu Verbrauchergerechtes statt. Wir berichten LIVE vor Ort.
#fairesscoring #openschufa

### Das wichtiges in Übersicht 

[Programm der Veranstaltung](https://www.bmjv.de/DE/Veranstaltungen/Anmeldeportal/Events/SVRV/SVRV_Anmeldung_node.html)

[Übersichtsseite zu „Verbrauchergerechtes Scoring“, von SVRV](http://www.svr-verbraucherfragen.de/dokumente/verbrauchergerechtes-scoring/)

[Technische und rechtliche Betrachtungen algorithmischer Entscheidungsverfahren
Gesellschaft für Informatik e. V.](http://www.svr-verbraucherfragen.de/dokumente/technische-und-rechtliche-betrachtungen-algorithmischer-entscheidungsverfahren/)

 





### Handlungsempfehlungen

1. Scoring für den Verbraucher verständlich machen
2. Scoring-Wissen und Kompetenz fördern
3. Diskriminierung prüfen und offenlegen
4. telematikfreie Option sicherstellen
5. Score-Qualität gewährleisten
6. Datenqualität sichern
7. Aufsicht verbessern
8. Super-Scores

1. 
"Ein Teil des SVRV spricht sich für eine weiter­
reichende Scoring-Transparenz aus. Er ist der
Auffassung, dass stets sämtliche in einen Score
eingehenden Merkmale gegenüber dem Verbrau­
cher offenzulegen sind und ihr relatives Gewicht
in der Score-Berechnung anzugeben ist."



### Aussagen

""Nicht nur in der Wissenschaft, auch in der Öffentlichkeit wird die Transparenz von Scoring-Verfahren diskutiert. Im Februar 2018 starteten die gemeinnützigen Organisationen Open Knowledge Foundation und Algorithm-Watch die Initiative OpenSCHUFA. Zu den erklärten Zielen des Projektes gehört es, den Algorithmus zu „knacken“, mit dem die Schufa ihre Bonitäts-scores ermittelt (OpenSCHUFA,  2018). Sowohl die in die Berechnung des Score-Wertes eingehenden Daten als auch das Verfahren, nach dem aus diesem Datenmaterial ein individueller Score ermittelt wird, will die Initiative herausfinden, indem möglichst viele Personen der Initiative ihren Schufa-Score (den sie dort erfragen) und ihre persönlichen Merkmale mitteilen. Zahlreiche Medien berichteten über das Anliegen der Initiative (exemplarisch Erdmann, 2018; Schneider, 2018), die SCHUFA selbst trat 
ihr kritisch entgegen (SCHUFA Holding AG, 2018)"




Wir verweisen auf die sieben Prinzipien des ACM (p 123 GI Studie)

Prinzipien für algorithmische Transparenz und Verantwortlichkeit sind folgende:
1. Awareness: Eigentümer, Designer, Benutzer und andere Beteiligte von Analysesystemen
sollten sich der möglichen Vorurteile bewusst sein, die mit ihrer Gestaltung, Implementierung
und Verwendung sowie dem möglichen Schaden, den Voreingenommenheit Einzelpersonen
und der Gesellschaft verursachen kann, verbunden sind.
2. Zugang und Rechtsbehelfe: Die Regulierungsbehörden sollten die Einführung von
Mechanismen fördern, die Befragungen und Rechtsbehelfe für Einzelpersonen und Gruppen
ermöglichen, die von algorithmisch fundierten Entscheidungen betroffen sind.
3. Rechenschaftspflicht: Institutionen sollten für Entscheidungen verantwortlich gemacht
werden, die durch die von ihnen verwendeten Algorithmen getroffen werden, auch wenn es
nicht möglich ist, im Detail zu erklären, wie die Algorithmen ihre Ergebnisse produzieren.
4. Erläuterung: Systeme und Institutionen, die algorithmische Entscheidungen treffen,
werden ermutigt, Erläuterungen sowohl zu den vom Algorithmus verfolgten Verfahren als
auch zu den spezifischen getroffenen Entscheidungen zu geben. Dies ist besonders wichtig
in öffentlichen politischen Kontexten.
5. Datenprovenienz: Eine Beschreibung der Art und Weise, in der die Trainingsdaten
gesammelt wurden, sollte von den Erstellern der Algorithmen beibehalten werden, begleitet
von einer Untersuchung der möglichen Verzerrungen, die durch den menschlichen oder
algorithmischen Datenerfassungsprozess induziert werden. Die öffentliche Überprüfung der
Daten bietet maximale Möglichkeiten für Korrekturen. Bedenken hinsichtlich des
Datenschutzes, des Schutzes von Geschäftsgeheimnissen oder der Offenlegung von
Analysen, die böswilligen Akteuren erlauben könnten, das System zu manipulieren, können
jedoch den Zugang auf qualifizierte und autorisierte Personen einschränken.
6. Überprüfbarkeit: Modelle, Algorithmen, Daten und Entscheidungen sollten aufgezeichnet
werden, damit sie in Fällen, in denen ein Schaden vermutet wird, überprüft werden können.
7. Validierung und Testen: Institutionen sollten strenge Methoden anwenden, um ihre
Modelle zu validieren und diese Methoden und Ergebnisse zu dokumentieren. Insbesondere
sollten sie routinemäßig Tests durchführen, um zu beurteilen und festzustellen, ob das
Modell diskriminierende Schäden verursacht. Die Institutionen werden ermutigt, die
Ergebnisse solcher Tests öffentlich zu machen.



### Wie geht es weiter bei OpenSCHUFA?

Veranstaltung im November ankündigen ... 
